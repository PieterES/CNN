# -*- coding: utf-8 -*-
"""Assignment_1.1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xs2wjMpxHj-DWoh5knTTy6l7Z-OnSHQP
"""

from tensorflow import keras
import numpy as np

import matplotlib.pyplot as plt

# Preprocessing Data
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (60000,784))
x_test = np.reshape(x_test, (10000,784))
x_train = x_train/225
x_test = x_test/225

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test,10)

#No activation NN

model1 = keras.Sequential()
model1.add(keras.layers.Dense(256, input_shape=(784,)))
model1.add(keras.layers.Dense(10, activation='softmax'))
model1.summary()
model1.compile(loss='categorical_crossentropy',
optimizer=keras.optimizers.RMSprop(), metrics='accuracy')
history1 = model1.fit(x_train, y_train, batch_size=128,epochs=12, verbose=1, validation_split=0.2)

# Plotting no activation NN
keys = history1.history.keys()
loss = history1.history['loss']
accuracy = history1.history['accuracy']
val_loss = history1.history['val_loss']
val_accuracy =  history1.history['val_accuracy']

plt.figure()
plt.plot(loss, label="Training Loss")
plt.plot(val_loss, label="Validation Loss")
plt.legend(loc="upper right")

plt.figure()
plt.plot(accuracy, label="Training Accuracy")
plt.plot(val_accuracy, label="Validation Accuracy")
plt.legend(loc="lower right")

print(model1.evaluate(x_test, y_test, verbose=0))

# Model with Relu activation function

model2 = keras.Sequential()
model2.add(keras.layers.Dense(256, input_shape=(784,), activation = 'relu'))
model2.add(keras.layers.Dense(10, activation='softmax'))
model2.summary()
model2.compile(loss='categorical_crossentropy',
optimizer=keras.optimizers.RMSprop(), metrics='accuracy')
history2 = model2.fit(x_train, y_train, batch_size=128,epochs=12, verbose=1, validation_split=0.2)

# Plotting Relu activation function model

keys = history2.history.keys()
loss = history2.history['loss']
accuracy = history2.history['accuracy']
val_loss = history2.history['val_loss']
val_accuracy =  history2.history['val_accuracy']

plt.figure()
plt.plot(loss, label="Training Loss")
plt.plot(val_loss, label="Validation Loss")
plt.legend(loc="upper right")


plt.figure()
plt.plot(accuracy, label="Training Accuracy")
plt.plot(val_accuracy, label="Validation Accuracy")
plt.legend(loc="lower right")
print(model2.evaluate(x_test, y_test, verbose=0))

# Data preprocssing

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train,(60000,28,28,1))
x_test = np.reshape(x_test, (10000,28,28,1))
x_train = x_train/225
x_test = x_test/225

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test,10)

#Convolutional 2D model

model3 = keras.Sequential()
model3.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation="relu", input_shape=(28, 28, 1)))
model3.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3),activation="relu"))
model3.add(keras.layers.MaxPool2D(pool_size=(2, 2)))
model3.add(keras.layers.Flatten())
model3.add(keras.layers.Dense(128, activation="relu"))
model3.add(keras.layers.Dense(10, activation="softmax"))
model3.summary()

model3.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adadelta(learning_rate=1), metrics='accuracy')
history3 = model3.fit(x_train, y_train, batch_size=128,epochs=6, verbose=1, validation_split=0.2)

# Plotting 2D convolutional model

keys = history3.history.keys()
loss = history3.history['loss']
accuracy = history3.history['accuracy']
val_loss = history3.history['val_loss']
val_accuracy =  history3.history['val_accuracy']

plt.figure()
plt.plot(loss, label="Training Loss")
plt.plot(val_loss, label="Validation Loss")
plt.legend(loc="upper right")


plt.figure()
plt.plot(accuracy, label="Training Accuracy")
plt.plot(val_accuracy, label="Validation Accuracy")
plt.legend(loc="lower right")
print(model3.evaluate(x_test, y_test, verbose=0))

#Dropout Model

model4 = keras.Sequential()
model4.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation="relu", input_shape=(28, 28, 1)))
model4.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3),activation="relu"))
model4.add(keras.layers.MaxPool2D(pool_size=(2, 2)))
model4.add(keras.layers.Dropout(rate=0.25))
model4.add(keras.layers.Flatten())
model4.add(keras.layers.Dense(128, activation="relu"))
model4.add(keras.layers.Dropout(rate=0.5))
model4.add(keras.layers.Dense(10, activation="softmax"))
model4.summary()

model4.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adadelta(learning_rate=1), metrics='accuracy')
history4 = model4.fit(x_train, y_train, batch_size=128,epochs=6, verbose=1, validation_split=0.2)

# Plotting Dropout model

keys = history4.history.keys()
loss = history4.history['loss']
accuracy = history4.history['accuracy']
val_loss = history4.history['val_loss']
val_accuracy =  history4.history['val_accuracy']

plt.figure()
plt.plot(loss, label="Training Loss")
plt.plot(val_loss, label="Validation Loss")
plt.legend(loc="upper right")


plt.figure()
plt.plot(accuracy, label="Training Accuracy")
plt.plot(val_accuracy, label="Validation Accuracy")
plt.legend(loc="lower right")
print(model4.evaluate(x_test, y_test, verbose=0))